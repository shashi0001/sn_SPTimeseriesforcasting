# -*- coding: utf-8 -*-
"""FinalExam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RSjMvMmM4OksXCG4GvmiDAxaJwEOH-1h

# 0.0 Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 0.1 Importing Libraries"""

import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.tsa.arima.model import ARIMA
from datetime import timedelta
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing
from prophet import Prophet

"""/content/drive/MyDrive/OPM6000/FinalExam

/content/drive/MyDrive/OPM6000/FinalExam/data

# Task 1

## 1. Loading all the csv files.
"""

df01 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Tomatoes.csv')
df02 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Sorghum.csv')
df03 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Potatoes (Irish).csv')
df04 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Peas (fresh).csv')
df05 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Oranges (big size).csv')
df06 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Maize.csv')
df07 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Chili (red).csv')
df08 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Cassava.csv')
df09 = pd.read_csv('/content/drive/MyDrive/OPM6000/FinalExam/data/Beans (dry).csv')

"""## 3. Consolidating the date column and parsing to date time object"""

# prompt: consolidate  mp_month and mp_year into a single columns as date in all the csv's

def consolidate_date(df):
    df['mp_year'] = df['mp_year'].astype(str)
    df['mp_month'] = df['mp_month'].astype(str).str.zfill(2)
    df['Date'] = pd.to_datetime(df['mp_year'] + '-' + df['mp_month'], format='%Y-%m')
    df = df.drop(['mp_year', 'mp_month'], axis=1)
    return df

df01 = consolidate_date(df01)
df02 = consolidate_date(df02)
df03 = consolidate_date(df03)
df04 = consolidate_date(df04)
df05 = consolidate_date(df05)
df06 = consolidate_date(df06)
df07 = consolidate_date(df07)
df08 = consolidate_date(df08)
df09 = consolidate_date(df09)

"""## 2. Extracting the name of the good"""

# prompt: concat mkt_name and cm_name into a new single column  as good in all the csv's

def add_combined_column(df):
    df['good'] = df['mkt_name'] + '_' + df['cm_name']
    return df

df01 = add_combined_column(df01)
df02 = add_combined_column(df02)
df03 = add_combined_column(df03)
df04 = add_combined_column(df04)
df05 = add_combined_column(df05)
df06 = add_combined_column(df06)
df07 = add_combined_column(df07)
df08 = add_combined_column(df08)
df09 = add_combined_column(df09)

df01.head()

"""## 4 & 5. Combining all the dataframes using date as index and each good in separate column and filling empty values with NaN"""

# prompt: combine all the dataframes into a single dataframe, while using date as index and different good having a different column

df010 = [df01, df02, df03, df04, df05, df06, df07, df08, df09]

# Concatenate all dataframes
combined_df01 = pd.concat(df010)

# Pivot the table
combined_df01 = combined_df01.pivot(index='Date', columns='good', values='mp_price')

combined_df01.head()

"""# Task 2"""

master_df = combined_df01.copy()

df011 = master_df.copy()

"""## 1. Data Overview"""

df011.shape

df011.info()

# prompt: List all the column names and their datatypes

df011.dtypes

# Check for missing dates within the date range
def check_date_continuity(df011):
    # Get the date range
    start_date = df011.index.min()
    end_date = df011.index.max()

    # Create a continuous date range
    date_range = pd.date_range(start=start_date, end=end_date, freq='MS')

    # Find missing dates
    missing_dates = date_range.difference(df011.index)

    if not missing_dates.empty:
        print("Missing dates:")
        print(missing_dates)
        return False
    else:
        print("Date range is continuous.")
        return True

check_date_continuity(df011)

"""## 2. Missing Values"""

# Identify missing values for each good
def missing_values_by_good(df011):
    missing_values_summary = df011.isnull().sum()
    return missing_values_summary

missing_values_summary = missing_values_by_good(df011)
missing_values_summary

# prompt: summarize the percentage of missing values per column

def summarize_missing_percentage(df011):
    missing_percentage = df011.isnull().sum() * 100 / len(df011)
    return missing_percentage

# Example usage with your existing DataFrame 'df011':
missing_percentage_summary = summarize_missing_percentage(df011)
missing_percentage_summary

"""## 3. Decsriptive Statistics"""

# prompt: Generate a summary statistics for each column (mean, median, min, max, and standard deviation)

def generate_summary_statistics(df011):
    if not isinstance(df011, pd.DataFrame) or df011.empty:
        return None

    summary_stats = df011.describe().loc[['mean', '50%', 'min', 'max', 'std']]
    summary_stats = summary_stats.rename(index={'50%': 'median'})
    return summary_stats

# Example usage with df011:
summary_stats_df011 = generate_summary_statistics(df011)
if summary_stats_df011 is not None:
  summary_stats_df011

summary_stats_df011

"""## 4. Time Series Visualizations"""

for col in df011.columns:
    plt.figure(figsize=(12, 6))
    plt.plot(df011.index, df011[col])
    plt.title(f'Time Series Plot for {col}')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.grid(True)
    plt.show()

# prompt: Overlay multiple time series in a single plot to explore relationships
# between goods in a better visualization easy to interpret with legend in a different image in an interactive graph

import matplotlib.pyplot as plt

# Assuming df011 is your DataFrame with 'Date' as index and goods as columns

# Create the plot
plt.figure(figsize=(12, 6))  # Adjust figure size as needed

for col in df011.columns:
    plt.plot(df011.index, df011[col], label=col)

plt.title('Time Series Plot of Multiple Goods')
plt.xlabel('Date')
plt.ylabel('Price')
plt.grid(True)

# Create a separate legend image
plt.figure(figsize=(2, 4))  # Adjust legend figure size as needed
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles, labels, loc='center')
plt.axis('off')  # Hide the axis for the legend image
plt.savefig('legend.png', bbox_inches='tight') # Save legend image

# Show the main plot (with no legend on it)
plt.figure(figsize=(12,6))
for col in df011.columns:
    plt.plot(df011.index, df011[col], label=col)
plt.title('Time Series Plot of Multiple Goods')
plt.xlabel('Date')
plt.ylabel('Price')
plt.grid(True)
plt.show()

# Display the saved legend image
from IPython.display import Image
Image('legend.png')


# Interactive plot using plotly (optional)
# Install plotly if you don't have it
# !pip install plotly

import plotly.graph_objects as go
from plotly.subplots import make_subplots

fig = go.Figure()
for col in df011.columns:
  fig.add_trace(go.Scatter(x=df011.index, y=df011[col], name=col, mode="lines+markers"))

fig.update_layout(title='Interactive Time Series Plot', xaxis_title='Date', yaxis_title='Price')

fig.show()

"""# Task 3"""

# Identify columns with missing values and their counts
missing_values = df011.isnull().sum()

# Filter for columns with missing values
missing_columns = missing_values[missing_values > 0]

print("Columns with missing values and their counts:")
missing_columns

# Fill missing values using linear interpolation for each column
df011_filled = df011.interpolate(method='linear', limit_direction='both')

"""Justification: Linear interpolation is generally a good choice for time series data when you have a relatively smooth trend. It assumes a linear relationship between data points and creates new values by estimating them along a straight line.
Setting `limit_direction='both'` ensures that missing values at the beginning and end of the time series are also filled using interpolation, propagating the values from the nearest available data points.
Alternative methods and when to use them:
Forward-fill (ffill):  Suitable when the value is likely to remain the
same as the previous value, such as in cases of sensor data, where measurements might be temporarily unavailable but expected to resume the previous value quickly.
Backward-fill (bfill): Suitable when the value is likely to remain the same as the next value, but this is less common.

# Task 4
"""

# prompt: Calculate the correlation matrix for the goods in the DataFrame df011_filled

correlation_matrix = df011_filled.corr()
correlation_matrix

# Find the pair of goods with the highest correlation (excluding self-correlation)
correlation_matrix = correlation_matrix.mask(np.equal(*np.indices(correlation_matrix.shape)))
max_value = np.nanmax(correlation_matrix)
row_max, col_max = correlation_matrix[correlation_matrix == max_value].stack().idxmax()

print(f"The pair of goods with the highest correlation is {row_max} and {col_max}, with a correlation of {max_value}")

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

# prompt: Justify why correlation is a meaningful metric for this comparison.

Correlation is a meaningful metric for comparing the price movements of different goods because it quantifies the linear relationship between their prices over time.  A high positive correlation indicates that the prices of two goods tend to move in the same direction—when the price of one increases, the price of the other tends to increase as well, and vice-versa. A high negative correlation suggests an inverse relationship—when the price of one increases, the price of the other tends to decrease.  A correlation near zero implies little to no linear relationship between the price movements.

In the context of your analysis, examining the correlation between different agricultural commodity prices helps to understand how they influence one another in the market.  This information can be valuable for:

* **Portfolio Management:**  Investors could use this information to diversify their investments in related commodities.
* **Market Prediction:**  Understanding the relationships between prices could lead to more accurate price forecasts.
* **Supply Chain Optimization:** Businesses operating in these markets can anticipate price fluctuations and optimize their purchasing and production strategies.
* **Economic Analysis:** Observing price correlations can give insights into the underlying economic factors that affect multiple commodities.


Correlation, while useful, does have limitations. It only captures *linear* relationships.  Two goods might have a complex, non-linear relationship that a correlation coefficient wouldn't fully reveal.  Additionally, correlation does not imply causation. Even if two goods are highly correlated, it doesn't necessarily mean that the price of one *causes* the price of the other to change.  Other factors might be influencing both.

"""# Task 5

## 1. Moving Average
"""

# prompt: Implement moving average on df011 where column is Gahanga_Tomatoes for 2 different window sizes (3 months and 6 months)

# Calculate 3-month moving average
df011['Gahanga_Tomatoes_MA_3M'] = df011['Gahanga_Tomatoes'].rolling(window=3).mean()

# Calculate 6-month moving average
df011['Gahanga_Tomatoes_MA_6M'] = df011['Gahanga_Tomatoes'].rolling(window=6).mean()

# Select the 'Gahanga_Tomatoes' column for prediction
data_to_predict = df011['Gahanga_Tomatoes']

# Fit an ARIMA model (example: order=(5,1,0))
model = ARIMA(data_to_predict, order=(5, 1, 0)) # Adjust the order as needed
model_fit = model.fit()

# Predict the next 6 months
last_date = data_to_predict.index[-1]
future_dates = [last_date + timedelta(days=30*i) for i in range(1, 7)] #add 6 months
predictions = model_fit.predict(start=len(data_to_predict), end=len(data_to_predict)+5)

# Create a DataFrame for the predictions
predictions_df = pd.DataFrame({'Date': future_dates, 'Predicted_Price': predictions})
predictions_df = predictions_df.set_index('Date')

# Print or visualize the predictions
print(predictions_df)

#Plot predictions
plt.figure(figsize=(12, 6))
plt.plot(data_to_predict.index, data_to_predict, label='Actual')
plt.plot(predictions_df.index, predictions_df['Predicted_Price'], label='Predicted')
plt.legend()
plt.show()

"""## 2. Exponential smoothing"""

# Simple Exponential Smoothing (SES)
ses_model = SimpleExpSmoothing(data_to_predict)
ses_fit = ses_model.fit(smoothing_level=0.2) # Adjust smoothing_level as needed
df011['Gahanga_Tomatoes_SES'] = ses_fit.fittedvalues

# Holt's Linear Trend Method
holt_model = ExponentialSmoothing(data_to_predict, trend="add") # Or "mul" for multiplicative trend
holt_fit = holt_model.fit(smoothing_level=0.2, smoothing_slope=0.1) # Adjust parameters as needed
df011['Gahanga_Tomatoes_Holt'] = holt_fit.fittedvalues

# Holt-Winters' Seasonal Method (if seasonality is present)
# hw_model = ExponentialSmoothing(data_to_predict, trend="add", seasonal="add", seasonal_periods=12) # 12 for yearly seasonality, adjust as needed
# hw_fit = hw_model.fit(smoothing_level=0.2, smoothing_slope=0.1, smoothing_seasonal=0.1)
# df011['Gahanga_Tomatoes_HW'] = hw_fit.fittedvalues

# Plotting to compare
plt.figure(figsize=(12, 6))
plt.plot(data_to_predict, label='Actual')
plt.plot(df011['Gahanga_Tomatoes_SES'], label='SES')
plt.plot(df011['Gahanga_Tomatoes_Holt'], label='Holt')
#plt.plot(df011['Gahanga_Tomatoes_HW'], label='Holt-Winters')  # uncomment if you use HW
plt.legend()
plt.show()

predictions_df

"""## 3. Facebook Prophet"""

# Assuming df011_filled is your DataFrame with the filled data

# Prepare the data for Prophet
prophet_df = df011_filled[['Gahanga_Tomatoes']].reset_index()
prophet_df.columns = ['ds', 'y']  # Rename columns to 'ds' and 'y'
prophet_df.head()

# prompt:  Fit a Prophet model and forecast the next 6 months.

# Initialize and fit the Prophet model
model = Prophet()
model.fit(prophet_df)

# Create future dataframe for 6 months
future = model.make_future_dataframe(periods=6, freq='MS')

# Make predictions
forecast = model.predict(future)

# Print the forecast
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(6))

"""## Visualization of all three models"""

# prompt:  Visualize the forecasted results for all three methods alongside the actual
# time series.

# Create a figure and axes for the plot
fig, ax = plt.subplots(figsize=(12, 6))

# Plot the actual time series
ax.plot(df011.index, df011['Gahanga_Tomatoes'], label='Actual', color='blue')

# Plot the ARIMA predictions
ax.plot(predictions_df.index, predictions_df['Predicted_Price'], label='ARIMA', color='red')

# Plot the exponential smoothing predictions
ax.plot(df011.index, df011['Gahanga_Tomatoes_SES'], label='SES', color='green')
ax.plot(df011.index, df011['Gahanga_Tomatoes_Holt'], label='Holt', color='purple')


# Plot the Prophet predictions
ax.plot(forecast['ds'], forecast['yhat'], label='Prophet', color='orange')


# Customize the plot
ax.set_xlabel('Date')
ax.set_ylabel('Price')
ax.set_title('Forecasted vs. Actual Prices for Gahanga Tomatoes')
ax.legend()
ax.grid(True)

# Show the plot
plt.show()